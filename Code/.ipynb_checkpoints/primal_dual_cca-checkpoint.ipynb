{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of primal dual CCA (aka partially linear CCA)\n",
    "Hardoon & Shawe-Taylor, 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T14:26:18.781117Z",
     "start_time": "2022-06-21T14:26:16.981046Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cvxpy as cp\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.optimize import minimize, NonlinearConstraint\n",
    "import scipy\n",
    "from sklearn.metrics.pairwise import pairwise_kernels, rbf_kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T14:26:51.243671Z",
     "start_time": "2022-06-21T14:26:50.616095Z"
    }
   },
   "outputs": [],
   "source": [
    "ret = pd.read_csv('ret_subset.csv')\n",
    "#kernel = np.array(pd.read_csv('kernel_matrix.csv', header=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T14:26:51.759282Z",
     "start_time": "2022-06-21T14:26:51.744287Z"
    }
   },
   "outputs": [],
   "source": [
    "universe_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T14:26:52.599801Z",
     "start_time": "2022-06-21T14:26:52.590801Z"
    }
   },
   "outputs": [],
   "source": [
    "ret = np.array(ret.iloc[:252,1:universe_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trading signals\n",
    "\n",
    "quick built trading signal to obtain a kernel to test our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T14:26:53.774539Z",
     "start_time": "2022-06-21T14:26:53.756538Z"
    }
   },
   "outputs": [],
   "source": [
    "def macd(ret, long=8, short=4, signal_span=9):\n",
    "    \"\"\"\n",
    "    calculates the MACD momentum strategy for a single time series\n",
    "    input: pandas single series\n",
    "    output: numpy array of signals\n",
    "    \"\"\"\n",
    "    short_signal = ret.ewm(span=short, adjust=False).mean()\n",
    "    long_signal = ret.ewm(span=long, adjust=False).mean() \n",
    "    macd = short_signal - long_signal\n",
    "    \n",
    "    pos = np.zeros(len(ret))\n",
    "    pos = np.where(macd>0,1,-1)\n",
    "    return pos\n",
    "\n",
    "def macd_signals(returns,long=26,short=12,signal_span=9):\n",
    "    \"\"\"\n",
    "    function calculating all the macd signals\n",
    "    input: pandas dataframe of returns\n",
    "    output: pandas dataframe of signals\n",
    "    \"\"\"\n",
    "    if type(returns) == np.ndarray:\n",
    "        returns = pd.DataFrame(returns)\n",
    "    signals = pd.DataFrame()\n",
    "    for i in range(returns.shape[1]):\n",
    "        signals['signal_{}'.format(i)] = macd(returns.iloc[:,i],long,short,signal_span)\n",
    "    signals.index = returns.index\n",
    "    return signals.shift(1).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# primal dual CCA\n",
    "exact implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T14:26:54.279448Z",
     "start_time": "2022-06-21T14:26:54.216446Z"
    }
   },
   "outputs": [],
   "source": [
    "def gaussian_kernel(X,s=1):\n",
    "    \"\"\"\n",
    "    function to obtain gaussian kernel\n",
    "    inputs:\n",
    "            X : numpy matrix of size assets x time\n",
    "            s : scale factor\n",
    "    \"\"\"\n",
    "    #pairwise_dists = squareform(pdist(X, 'euclidean'))\n",
    "    #K = scipy.exp(-pairwise_dists ** 2 / s ** 2)\n",
    "    \n",
    "    # K(x, y) = exp(-gamma ||x-y||^2)\n",
    "    K = rbf_kernel(X, gamma=s)\n",
    "    return K\n",
    "\n",
    "def scca(X,K,masterI,outdis,debug,sk):\n",
    "    \"\"\"\n",
    "    CURRENT VERSION! 1.06\n",
    "\n",
    " [w,e,alpha,beta,mu,gamma,cor,res] = SCCA2(X,K,masterI,od,debug,sk)\n",
    "\n",
    " Sparse Canonical Correlation Analysis - SCCA, is a primal-dual solver for\n",
    " the CCA problem. Given primal data of a view and a dual representation of\n",
    " the second view will provide a sparse primal weight vector (for the primal\n",
    " data) and sparse feature projection (for the dual [kernel] data)\n",
    "\n",
    " Input:  X        - Primal data of view one    [m x l]\n",
    "         K        - dual data of view two      [l x l]\n",
    "         masterI  - Starting point for e       [1 x 1]\n",
    "         od       - output display true/false\n",
    "         debug    - outputs primal-dual progression true/false\n",
    "         sk       - scaling factor for mu and gamma\n",
    "\n",
    " Output: w     - sprase weight vector      [1 x m]\n",
    "         e     - sparse projct vectors     [1 x l]\n",
    "         alpha - 1'st view dual parameters [1 x m]\n",
    "         beta  - 2'nd view dual parameters [1 x l]\n",
    "         mu    - regularsation parameter   [1 x 1]\n",
    "         gamma - lagrangian (scale factor) [1 x 1]\n",
    "         cor   - correlation value         [1 x 1]\n",
    "         res   - Optimisation solution/s   [1 x 1]\n",
    "\n",
    "\n",
    " Written by David R. Hardoon 25/06/2007\n",
    " http://homepage.mac.com/davidrh/\n",
    " D.Hardoon@cs.ucl.ac.uk\n",
    "\n",
    " Further updats done - 15/04/2008\n",
    "\n",
    " Email the author any modification that are applied to the original code.\n",
    "\n",
    " No commercial usage is allowed.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    #Initialising parameters \n",
    "    #Setting the infinity norm\n",
    "    e = np.zeros(K.shape[1])\n",
    "    beta = e\n",
    "    e[masterI] = 1\n",
    "    sa_e = e\n",
    "    eDifference = 0\n",
    "    \n",
    "\n",
    "    #So we don't need to recomput do once.    \n",
    "    c = X @ (K[:,masterI] * e[masterI])\n",
    "    KK = K @ K\n",
    "\n",
    "    # More setting up initial parameters\n",
    "    N = X.shape[0]\n",
    "    w = np.zeros(N)\n",
    "    sa_w = w.copy()\n",
    "    j = np.ones(N)\n",
    "    \n",
    "    #So that we do not use the e_i\n",
    "    Ij = np.eye(K.shape[1])\n",
    "    Ij[masterI,masterI] = 0\n",
    "    \n",
    "\n",
    "    #Setting initial tolerance values\n",
    "    etolerance = 0.01\n",
    "    globtolerance = 1e-5\n",
    "\n",
    "    #Set trade-off to half\n",
    "    tau = 0.5\n",
    "\n",
    "    # Setting the mu and gamma regularsation parameters\n",
    "    d1 = 2*tau*(1-tau)*c # The reminder of the equation is zero\n",
    "    mu = sk*np.mean(abs(d1))\n",
    "    \n",
    "    gamma = np.mean(abs(2 * (1-tau)**2 * Ij @ KK[:,masterI]*e[masterI]))\n",
    "    # Computing the upper bound on the w's\n",
    "    C = 2*mu\n",
    "\n",
    "    # Computing inital alpha\n",
    "    alpha = d1 + mu*j;\n",
    "    \n",
    "    # Finding alphas that break the constraints\n",
    "    I1 = np.where(alpha < 0)[0]\n",
    "    I2 = np.where(alpha > C)[0]\n",
    "    I = np.sort(np.concatenate([I1,I2]))\n",
    "\n",
    "    # Selecting the violations\n",
    "    ta = alpha[I]\n",
    "    ta[ta>0] = ta[ta>0] - C\n",
    "    ta = abs(ta)\n",
    "    \n",
    "    stai = np.argsort(ta)\n",
    "    I = I[stai]\n",
    "\n",
    "    if len(I) > 1000:\n",
    "        I = I[(len(I)-999):]\n",
    "\n",
    "    pI = I\n",
    "\n",
    "    # Initial W tolerance is set\n",
    "    tolerance = 0.3*abs(max(alpha[I]))\n",
    "    \n",
    "    if outdis==1:\n",
    "        print(\"Selected regularisation value; mu = {}, gamma = {}\".format(mu,gamma))\n",
    "    \n",
    "\n",
    "    # We don't need to work on all of e\n",
    "    J = np.where(e != 0)\n",
    "\n",
    "    # Remembering the alpha violations\n",
    "    preAlpha = alpha[I]\n",
    "    \n",
    "    # Initially the difference will be zero\n",
    "    alphaDifference = abs(alpha[I] - preAlpha)\n",
    "    \n",
    "    # Flag on whether to exit\n",
    "    exitLoop = 1\n",
    "\n",
    "    # Loop counter\n",
    "    wloop = 1\n",
    "\n",
    "    # Do we need to compute the covariance?\n",
    "    skipCon = 0\n",
    "\n",
    "    # Do we need to go over all of e, to find new violations\n",
    "    completeE = 1\n",
    "    loo = 1\n",
    "\n",
    "    # The loop is repeated while there are violations and we are still working\n",
    "    # on the alphas and that the difference between the pervious and corrent\n",
    "    # alphas is minimal\n",
    "    while((I.any() and exitLoop) | (sum((alphaDifference > globtolerance) == 1) == 0)):\n",
    "        # set change to true so we enter the convergence on w\n",
    "        change = True\n",
    "        N = len(I)\n",
    "        \n",
    "        # compute the new covariance matrix if needed\n",
    "        if (skipCon == 0):\n",
    "            CX = X[I,:] @ X[I,:].T\n",
    "            \n",
    "        # save the previous alphas\n",
    "        preAlpha = alpha[I]\n",
    "        \n",
    "        # until convergence do\n",
    "        while(change):\n",
    "            # we can exit\n",
    "            change = False\n",
    "            \n",
    "            # setting the update\n",
    "            lefts = CX @ w[I]\n",
    "            \n",
    "            # for the found alphas\n",
    "            for i in range(N):\n",
    "                # upper and lower bounding alpha\n",
    "                needtoupdate1 = False\n",
    "                \n",
    "                if(alpha[I[i]] > C):\n",
    "                    alpha[I[i]] = C\n",
    "                    needtoupdate1 = True\n",
    "                elif (alpha[I[i]] < 0):\n",
    "                    alpha[I[i]] = 0\n",
    "                    needtoupdate1 = True\n",
    "                else:\n",
    "                    # if alpha is between the bound values\n",
    "                    # shift w if needed\n",
    "                    if (w[I[i]] > 0):\n",
    "                        dw = (C-alpha[I[i]]) / (2 * tau**2 *CX[i,i])\n",
    "                        w[I[i]] = w[I[i]] - dw\n",
    "                    elif (w[I[i]] < 0):\n",
    "                        dw = alpha[I[i]] / (2 * tau**2 * CX[i,i])\n",
    "                        w[I[i]] = w[I[i]] + dw\n",
    "                \n",
    "                # update w if needed to\n",
    "                if (needtoupdate1==True):\n",
    "                    # computing the learning rate\n",
    "                    learningRate = 1 / (2 * tau**2 * CX[i,i])\n",
    "                \n",
    "                    # updating\n",
    "                    firstBit = 2 * tau * (1-tau) * c[I[i]] + mu - alpha[I[i]]\n",
    "                    w[I[i]] = w[I[i]] + learningRate * (firstBit - 2 * tau**2 * lefts[i])\n",
    "                    \n",
    "                # Checking that w does not skip zero\n",
    "                if ((sa_w[I[i]] < 0 and w[I[i]] > 0) or (sa_w[I[i]] > 0 and w[I[i]] < 0)):\n",
    "                    w[I[i]] = 0\n",
    "                \n",
    "                # computing change\n",
    "                b = w[I[i]]-sa_w[I[i]]\n",
    "                sa_w[I[i]] = w[I[i]]\n",
    "                \n",
    "                if b != 0:\n",
    "                    lefts = lefts + CX[:,i] * b\n",
    "                \n",
    "                # computing the new lagrangian\n",
    "                alpha[I] = 2 * tau * (1-tau) * c[I] + mu - 2 * tau**2 * lefts\n",
    "                \n",
    "                # did we converge enough?\n",
    "                if abs(b) > tolerance:\n",
    "                    change = True\n",
    "            \n",
    "            #for loop ident\n",
    "        #while change loop ident\n",
    "        \n",
    "        ########################################\n",
    "        # working on the e's now\n",
    "        \n",
    "        # check whether we need to even waste time on e\n",
    "        if K.shape[1] > 1:\n",
    "            # compute all beta's (since beta are taking into account as a shadow\n",
    "            # variable i.e. they are not really computed, we are able to use their\n",
    "            # value as an indication of which e's are needed)\n",
    "            local_beta = 2 * (1-tau)**2 * Ij @ (KK @ e) - 2 * tau * (1-tau) * Ij @ K.T  @ X[I,:].T  @ w[I] + gamma\n",
    "            \n",
    "            # find e's that need to be worked on\n",
    "            J = np.sort(np.append(np.where(local_beta < 0), masterI))\n",
    "            \n",
    "            # save previous e's\n",
    "            preE = e[J]\n",
    "            \n",
    "            # precompute part of lagrangian update\n",
    "            oneP = 2 * tau * (1-tau) * Ij[J,J].T  @ K[:,J].T @ X[I,:].T @ w[I]\n",
    "\n",
    "            # converging over e\n",
    "            change = True\n",
    "            N = J.shape[0]\n",
    "            \n",
    "            while(change==True):\n",
    "                change = False\n",
    "                \n",
    "                lefts = Ij[J,J].T * KK[J,J] * e[J]\n",
    "                \n",
    "                for i in range(N):\n",
    "                    if (J[i] != masterI) :\n",
    "                        learningRate = 1 / (4 * (1-tau)**2 * KK[J[i],J[i]])\n",
    "\n",
    "                        if (learningRate > 1e+3 or learningRate < 1e-3):\n",
    "                            learningRate=1\n",
    "                        # before : oneP[i]\n",
    "                        e[J[i]] = e[J[i]] + learningRate*(oneP - 2 * (1-tau)**2 * lefts[i] + beta[J[i]] - gamma)\n",
    "\n",
    "                        if (e[J[i]] < 0):\n",
    "                            e[J[i]] = 0\n",
    "                        elif (e[J[i]] > 1):\n",
    "                            e[J[i]] = 1\n",
    "                        else:\n",
    "                            beta[J[i]] = 0\n",
    "\n",
    "                        b = e[J[i]]-sa_e[J[i]]\n",
    "                        sa_e[J[i]] = e[J[i]]\n",
    "\n",
    "                        if b != 0:\n",
    "                            lefts = lefts + Ij[J,J].T @ KK[J,J[i]] * b\n",
    "\n",
    "                            if abs(b) > etolerance:\n",
    "                                change = True\n",
    "            # recompute c\n",
    "            c = X @ (K[:,J] @ e[J])\n",
    "            \n",
    "            # check to see if there is any difference from previous e's\n",
    "            eDifference = abs(e[J] - preE)\n",
    "            \n",
    "            # compute new tolerance values\n",
    "            etolerance = 0.3 * abs(max(eDifference))\n",
    "            \n",
    "            # bound the tolerance values\n",
    "            if (etolerance==0 or etolerance < globtolerance):\n",
    "                etolerance = globtolerance\n",
    "        \n",
    "        \n",
    "        # recompute alpha using the new w's        \n",
    "        alpha = 2 * tau * (1-tau) * c + mu*j - 2 * tau**2 * X @ (X[I,:].T @ w[I])\n",
    "        \n",
    "        # check to see if there is any difference from previous alpha's (e's)\n",
    "        alphaDifference = abs(alpha[I] - preAlpha)\n",
    "        \n",
    "        # compute new tolerance values\n",
    "        tolerance = 0.3*abs(max(alphaDifference))\n",
    "        \n",
    "        if (tolerance == 0 or tolerance < globtolerance):\n",
    "            tolerance = globtolerance\n",
    "        \n",
    "        if debug :\n",
    "            print('Loop number {}'.format(wloop))\n",
    "            print('Tolerance value = {}'.format(tolerance))\n",
    "            print('Error value = {}'.format(sum(alphaDifference)))\n",
    "            print('Etolerance value = {}'.format(etolerance))\n",
    "            print('Error evalue = {}'.format(sum(eDifference)))\n",
    "        \n",
    "        # find alphas that break the constraint\n",
    "        markI = I.copy()\n",
    "        skipCon = 0\n",
    "        \n",
    "        I1 = np.where(alpha + globtolerance < 0)[0]\n",
    "        I2 = np.where(alpha - globtolerance > C)[0]\n",
    "        I = np.sort(np.concatenate([I1,I2]))\n",
    "        \n",
    "        # breakout if need to\n",
    "        if (I.any()==True) : \n",
    "            exitLoop = 1\n",
    "            # selecting the maximum nf violations\n",
    "            ta = alpha[I]\n",
    "            ta[ta>0] = ta[ta>0] - C\n",
    "            ta = abs(ta)\n",
    "            \n",
    "            # sorting as to select the largest violations first\n",
    "            stai = np.argsort(ta)\n",
    "            I = I[stai]\n",
    "            \n",
    "            # sanity check - are any of the violations repeats?           \n",
    "            for kp in range(len(I)):\n",
    "                \n",
    "                lc = np.where(I[kp]==pI)[0]\n",
    "                \n",
    "                if lc > -10:\n",
    "                    pI[lc] = -10\n",
    "           \n",
    "        \n",
    "            # grab only one copy of the violations\n",
    "            pI = pI[[pI != -10]]            \n",
    "            \n",
    "            # adding the previous I's for which w has a non zero element\n",
    "            np.sort(np.append(np.where(local_beta < 0), masterI))\n",
    "            I = np.sort(np.append(pI[w[pI] != 0], I))\n",
    "            if len(I) > 1000:\n",
    "                I = I[(len(I) - 999):]\n",
    "            # check to see if we need to compute the covariance matrix again\n",
    "            tmp1 = sum( np.tile(markI.T, (len(I), 1)) == np.tile(I, (len(markI),1)).T )\n",
    "            \n",
    "            if (sum(tmp1) == len(tmp1) and len(I) == len(markI)):\n",
    "                skipCon = 1\n",
    "            \n",
    "            # saving the current index\n",
    "            pI = I\n",
    "            \n",
    "        else:\n",
    "            # no violations, we can potentially exit the algorithm\n",
    "            exitLoop = 0\n",
    "            I = pI\n",
    "        \n",
    "        # update loop number\n",
    "        wloop += 1\n",
    "    #############################################################################\n",
    "    # end of convergence algorithm\n",
    "    \n",
    "    # compute vector length\n",
    "    wv = (w @ X) @ (X.T @ w)\n",
    "    ev = e @ KK @ e\n",
    "    \n",
    "    # normalize e\n",
    "    e = e / np.sqrt(ev)\n",
    "    \n",
    "    # normalize w, but check that we found something\n",
    "    if sum(w != 0) > 0:\n",
    "        w = w/np.sqrt(wv)\n",
    "    \n",
    "    # compute the optimisation error value\n",
    "    res = np.linalg.norm(tau * X.T @ w - (1-tau) * K @ e)**2\n",
    "    \n",
    "    # compute the correlation value\n",
    "    cor = w @ X @ K @ e\n",
    "    \n",
    "    if outdis == 1:\n",
    "        print('----------------------------------------------- \\n')\n",
    "        print('we have {} non zero weights'.format(sum(w != 0)))\n",
    "        print('and {} non zero dual weights'.format(sum(e !=0)))\n",
    "        print('correlation = {}'.format(cor))\n",
    "        print('mu = {}'.format(mu))\n",
    "        print('gamma = {}'.format(gamma))\n",
    "        tmp = e[masterI]\n",
    "        e[masterI] = 0\n",
    "        print('|e|1 = {}'.format(np.linalg.norm(e,1)))\n",
    "        e[masterI] = tmp\n",
    "        print('e*KK*e = {}, w*X*X*w = {}'.format(ev, wv))\n",
    "        print('----------------------------------------------------')\n",
    "    \n",
    "    \n",
    "    return w,e,alpha,beta,mu,gamma,cor,res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T14:26:54.745852Z",
     "start_time": "2022-06-21T14:26:54.715382Z"
    }
   },
   "outputs": [],
   "source": [
    "def scca_deflator(trainX,Kb,k,a,b,c):\n",
    "    \"\"\"\n",
    "    \n",
    "     This is a wrapper function that runs the SCCA2 function while deflating\n",
    "     and finding new projection directions\n",
    "    \n",
    "     Input:\n",
    "       trainX - first view train data\n",
    "       Kb     - second view train data\n",
    "       k      - vector of indices for deflations\n",
    "       a,b    - output and debug variables for scca\n",
    "    \n",
    "     Output:\n",
    "       W      - Projection for primal\n",
    "       Z      - Projection for dual\n",
    "       output - a struct with various values\n",
    "    \n",
    "     Written by David R. Hardoon\n",
    "     UCL D.Hardoon@cs.ucl.ac.uk\n",
    "    \"\"\"\n",
    "    tX = trainX.copy()\n",
    "    KK = Kb.copy()\n",
    "    co = 0\n",
    "    \n",
    "    # initialise temporary variables\n",
    "    wa = np.zeros((trainX.shape[0],len(k)))\n",
    "    e = np.zeros((Kb.shape[1], len(k)))\n",
    "    resval = np.zeros(len(k))\n",
    "    corval = np.zeros(len(k))\n",
    "    projk = np.zeros((Kb.shape[1], len(k)))\n",
    "    tau = np.zeros((Kb.shape[1], len(k)))\n",
    "    proj = np.zeros((trainX.shape[0], len(k)))\n",
    "    t = np.zeros((Kb.shape[1], len(k)))\n",
    "    \n",
    "    for i in range(len(k)):\n",
    "        print('.')\n",
    "        output_w,output_e,t1,t2,t3,t4,output_cor,output_res = scca(tX,KK,k[i],a,b,c)\n",
    "        \n",
    "        wa[:,co] = output_w\n",
    "        e[:,co] = output_e\n",
    "        resval[co] = output_res\n",
    "        corval[co] = output_cor\n",
    "        \n",
    "        co += 1\n",
    "        \n",
    "        # dual deflation\n",
    "        projk[:,i] = KK @ e[:,i]\n",
    "        tau[:,i] = KK @ projk[:,i]        \n",
    "        \n",
    "        P = np.eye(len(KK)) - (np.outer(tau[:,i], tau[:,i])) / (tau[:,i].T @ tau[:,i])\n",
    "        KK = P.T @ KK @ P\n",
    "        \n",
    "        # primal deflation\n",
    "        proj[:,i] = tX @ (tX.T @ wa[:,i])\n",
    "        t[:,i] = tX.T @ proj[:,i]\n",
    "        tX = tX - tX @ (np.outer(t[:,i], t[:,i])) / (t[:,i].T @ t[:,i])\n",
    "    print('    ')\n",
    "\n",
    "    # Primal projection\n",
    "    P = trainX @ t @ np.linalg.inv(t.T @ t)\n",
    "    W = proj @ np.linalg.inv(P.T @ proj)\n",
    "\n",
    "    # can't think of a fancy way to normalise the vectors\n",
    "    WW = W.copy()\n",
    "    for i in range(W.shape[1]):\n",
    "        WW[:,i] = W[:,i] / np.linalg.norm(trainX.T @ W[:,i])\n",
    "    W = WW\n",
    "\n",
    "    # Dual Projection\n",
    "    Z = projk @ np.linalg.inv(np.linalg.inv(tau.T @ tau) @ tau.T@ Kb @ projk)\n",
    "    \n",
    "    ZZ = Z.copy()\n",
    "    for i in range(Z.shape[1]):\n",
    "        ZZ[:,i] = Z[:,i] / np.linalg.norm(Kb @ Z[:,i])\n",
    "    Z = ZZ\n",
    "    \n",
    "    output = {\"w\" : wa,\n",
    "             \"e\" : e,\n",
    "             \"P\" : P,\n",
    "             \"dual_tau\" : tau,\n",
    "             \"primal_tau\" : t,\n",
    "             \"cor\" : corval,\n",
    "             \"res\" : resval,\n",
    "             \"W\" : W,\n",
    "             \"Z\" : Z} \n",
    "    return W, Z, output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T14:26:55.650437Z",
     "start_time": "2022-06-21T14:26:55.589586Z"
    }
   },
   "outputs": [],
   "source": [
    "s = macd_signals(ret)\n",
    "tr_ret = np.array(s) * ret\n",
    "kernel = gaussian_kernel(tr_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T14:27:19.809389Z",
     "start_time": "2022-06-21T14:27:19.455014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      ".\n",
      ".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harol\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:329: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "C:\\Users\\Harol\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:334: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      ".\n",
      "    \n",
      "[0.3407347  0.61621103 0.45990806 0.47786721 0.5178738 ]\n"
     ]
    }
   ],
   "source": [
    "W,Z, output = scca_deflator(ret.T,kernel,np.arange(0,5,1),0,0,0.1)\n",
    "print(output['cor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T14:27:23.643252Z",
     "start_time": "2022-06-21T14:27:23.550819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected regularisation value; mu = 0.01524987585493117, gamma = 98.12386378312834\n",
      "Loop number 1\n",
      "Tolerance value = 0.1360844855958673\n",
      "Error value = 7.046915667460405\n",
      "Etolerance value = 1e-05\n",
      "Error evalue = 0.0\n",
      "Loop number 2\n",
      "Tolerance value = 0.04537462930976521\n",
      "Error value = 0.3740768332071668\n",
      "Etolerance value = 1e-05\n",
      "Error evalue = 0.0\n",
      "Loop number 3\n",
      "Tolerance value = 0.003775162782706298\n",
      "Error value = 0.10487061134352604\n",
      "Etolerance value = 1e-05\n",
      "Error evalue = 0.0\n",
      "Loop number 4\n",
      "Tolerance value = 0.0012096833828797884\n",
      "Error value = 0.07118012787049666\n",
      "Etolerance value = 1e-05\n",
      "Error evalue = 0.0\n",
      "Loop number 5\n",
      "Tolerance value = 0.0002285462310851749\n",
      "Error value = 0.01118457358544696\n",
      "Etolerance value = 1e-05\n",
      "Error evalue = 0.0\n",
      "Loop number 6\n",
      "Tolerance value = 5.802263098376309e-05\n",
      "Error value = 0.002318800506294313\n",
      "Etolerance value = 1e-05\n",
      "Error evalue = 0.0\n",
      "Loop number 7\n",
      "Tolerance value = 1.1114611554585707e-05\n",
      "Error value = 0.0004257963334659831\n",
      "Etolerance value = 1e-05\n",
      "Error evalue = 0.0\n",
      "----------------------------------------------- \n",
      "\n",
      "we have 47 non zero weights\n",
      "and 1 non zero dual weights\n",
      "correlation = 0.3418380517336209\n",
      "mu = 0.01524987585493117\n",
      "gamma = 98.12386378312834\n",
      "|e|1 = 0.0\n",
      "e*KK*e = 195.3709238917073, w*X*X*w = 19.020273467475988\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harol\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:329: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "C:\\Users\\Harol\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:334: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    }
   ],
   "source": [
    "# verbose example of single portfolio\n",
    "results1 = scca(np.array(ret).T,kernel,5,1,1,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc\n",
    "Hardoon & Shawe-Taylor 2011, non-exact implementation, based on https://github.com/aalto-ics-kepaco/primal_dual_scca\n",
    "\n",
    "Method does not converge correctly I believe, but I leave the implementation here for further reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-21T13:21:53.746621Z",
     "start_time": "2022-06-21T13:21:53.722592Z"
    }
   },
   "outputs": [],
   "source": [
    "def primal_dual_cca(X, K, seed_index, sk):\n",
    "    \"\"\"\n",
    "     Original description by David R. Hardoon: \n",
    "     Sparse Canonical Correlation Analysis - SCCA, is a primal-dual solver for\n",
    "     the CCA problem. Given primal data of a view and a dual representation of\n",
    "     the second view will provide a sparse primal weight vector (for the primal\n",
    "     data) and sparse feature projection (for the dual [kernel] data)\n",
    "\n",
    "     Input:  X             - Primal data of view one    [m x l] (rows is the number of assets)\n",
    "             K             - dual data of view two      [l x l]\n",
    "             seed_index    - Starting point for e       [1 x 1]\n",
    "             sk            - scaling factor for mu and gamma\n",
    "\n",
    "     Output: w             - sparse weight vector      [1 x m]\n",
    "             e             - sparse projct vectors     [1 x l]\n",
    "             cor           - correlation value         [1 x 1]\n",
    "    \"\"\"\n",
    "    primal_dim = X.shape[0]\n",
    "    N_samples = X.shape[1]\n",
    "    tau = 0.5\n",
    "\n",
    "    #This is how mu and gamma are set in David's SCCA2.m\n",
    "    Ij = np.zeros((K.shape[1], K.shape[1]))\n",
    "    np.fill_diagonal(Ij, 1)\n",
    "    Ij[seed_index, seed_index] = 0\n",
    "    c = X * K[:,seed_index]\n",
    "    KK = np.transpose(K) * K\n",
    "    d1 = 2*tau*(1-tau)*c\n",
    "    mu = sk*np.mean(np.abs(d1))\n",
    "    gamma = np.mean(np.abs(2*(1-tau)**2*Ij*KK[:,seed_index]))\n",
    "    beta = 1\n",
    "    \n",
    "    # initial parameters\n",
    "    w = np.zeros(primal_dim)\n",
    "    e = np.zeros(N_samples)\n",
    "    e[seed_index] = 1\n",
    "    initial = np.concatenate([w,e])\n",
    "    \n",
    "    # bounds\n",
    "    bnds  = [(-np.inf,np.inf) for i in range(primal_dim)]\n",
    "    bnds2 = [(0, None) for i in range(primal_dim,N_samples+primal_dim)]\n",
    "    bnds.extend(bnds2)\n",
    "\n",
    "    # constraints\n",
    "    const = NonlinearConstraint(kernel_weights_constraint,1.0,1.0)\n",
    "    \n",
    "    # minimization\n",
    "    result = minimize(pl_minimize,x0=initial, args = (X,K,tau,beta,gamma,mu,primal_dim),bounds=bnds, constraints=const).x#, bounds = bnds)#, constraints =(const,))\n",
    "    \n",
    "    w = result[:primal_dim]\n",
    "    e = result[primal_dim:]\n",
    "    p1 = w @ X @ X.T @ w\n",
    "    p2 = e @ K @ K @ e\n",
    "    corr = w @ X @ K @ e / np.sqrt(p1*p2)\n",
    "    return w,e,corr\n",
    "    \n",
    "def kernel_weights_constraint(x):\n",
    "    \"\"\"\n",
    "    need to define a global variable for the dimension of our asset space, currently it is 19\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(x[19:],np.inf)\n",
    "\n",
    "def pl_minimize(x, *args):\n",
    "    X, K, tau, beta, gamma, mu, dimension = args\n",
    "    w = x[:dimension]\n",
    "    e = x[dimension:]\n",
    "    res = np.linalg.norm(tau * X.T @ w - (1-tau)*K @ e) + mu*np.linalg.norm(w,1) + gamma*np.linalg.norm(e,1)\n",
    "    return np.maximum(res, np.zeros(res.shape))**2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
