{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This file will serve as the main testing ground for a variety of signals for the CRSP data set\n",
    "\n",
    "## The multiple strategies are the underlying:\n",
    "\n",
    "### 1. Univariate Strategies\n",
    "   * MACD (momentum), bollinger bands (mean reversion), univariate KRR\n",
    "   \n",
    "### 2. Multivariate Strategies\n",
    "   * multivariate KRR, VAR\n",
    "   \n",
    "### 3. NN-based methodologies\n",
    "   \n",
    "### 4. Others? (eg. from academic literature)\n",
    "\n",
    "### Further to do:\n",
    "\n",
    "   * **ideas to include:**\n",
    "       * PLS (pure dimension reduction but keeping objective), elastic net (as a way to include penalization methods)\n",
    "       * momentum:\n",
    "           * p.138 algo trading book: fama & blume 1966, Moskowitz Yao & Pedersen 2012 adaptation\n",
    "       * pairs trading strategy using cointegration\n",
    "       * mean reversion: individual (p.48 algo trading ernest chan) and in pairs (cointegrated assets: short one, long other, with and without bollinger bands, using kalman filters p.67,71,78)\n",
    "       * seasonal trending strategy (buy yearly losers end of december and sell them end of january) -> Singal 2006\n",
    "   * delve deeper into cross validation methodologies for each method, making sure all parameters are optimal\n",
    "   * include fractional differentiation to maximise memory & keep stationarity\n",
    "   * perform necessary statistical tests on VAR & its parameters\n",
    "        * get VAR to work only with cointegrated pairs\n",
    "   * generally: optimize code & parallelize operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T08:28:55.887406Z",
     "start_time": "2022-05-30T08:28:55.868410Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import statsmodels as sm\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.kernel_ridge import *\n",
    "from sklearn import linear_model\n",
    "import math\n",
    "import scipy\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import coint, adfuller\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.eval_measures import rmse, aic\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from scipy.stats.mstats import winsorize\n",
    "from scipy.stats import chi2\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T08:28:58.151675Z",
     "start_time": "2022-05-30T08:28:57.512078Z"
    }
   },
   "outputs": [],
   "source": [
    "data  = pd.read_csv('data/returns.csv')\n",
    "data.set_index('date', inplace=True)\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data = data.iloc[:,:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate strategies\n",
    "* Simple strategies\n",
    "    + mean reversion -> DONE\n",
    "    + momentum (MACD) -> DONE\n",
    "* forecasting strategies (requires cross-validation procedure)\n",
    "    + ARMA\n",
    "    + KRR (extremely slow) -> DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T08:30:02.935560Z",
     "start_time": "2022-05-30T08:30:02.906562Z"
    }
   },
   "outputs": [],
   "source": [
    "def momentum(ret):\n",
    "    \"\"\"\n",
    "    This function is based on the Moskowitz et al. paper from 2012, whereby we go long for 1 month\n",
    "    if the last 12 months were positive, in the other case, we go short.\n",
    "    \n",
    "    input: daily returns pandas dataframe of multivariate returns\n",
    "    output: buy/sell signals dataframe\n",
    "    \"\"\"\n",
    "    monthly_ret = ret.resample('M').sum()\n",
    "    signal = pd.DataFrame(columns=monthly_ret.columns, index=monthly_ret.index)\n",
    "        \n",
    "    for m in range(13,monthly_ret.shape[0]):\n",
    "        year = monthly_ret.iloc[(m-13):(m-1),:]\n",
    "        yearly_ret = np.sort(np.array(year.sum(axis=0)))[::-1] # sort in descending order\n",
    "        s = np.where(year.sum(axis=0) > 0, 1, 1)\n",
    "        signal.iloc[m,:] = s\n",
    "    \n",
    "    signal = signal.replace(np.nan, 0)\n",
    "    dates = ret.index\n",
    "    dates.name = 'date'\n",
    "    signal = signal.reindex(dates, method='bfill') # re-index to daily\n",
    "    return signal\n",
    "    \n",
    "\n",
    "def macd(ret, long=8, short=4, signal_span=9):\n",
    "    \"\"\"\n",
    "    calculates the MACD momentum strategy for a single time series\n",
    "    input: pandas single series\n",
    "    output: numpy array of signals\n",
    "    \"\"\"\n",
    "    short_signal = ret.ewm(span=short, adjust=False).mean()\n",
    "    long_signal = ret.ewm(span=long, adjust=False).mean() \n",
    "    macd = short_signal - long_signal\n",
    "    \n",
    "    pos = np.zeros(len(ret))\n",
    "    pos = np.where(macd>0,1,-1)\n",
    "    return pos\n",
    "\n",
    "def macd_signals(returns,long=26,short=12,signal_span=9):\n",
    "    \"\"\"\n",
    "    function calculating all the macd signals\n",
    "    input: pandas dataframe of returns\n",
    "    output: pandas dataframe of signals\n",
    "    \"\"\"\n",
    "    signals = pd.DataFrame()\n",
    "    for i in range(returns.shape[1]):\n",
    "        signals['signal_{}'.format(i)] = macd(returns.iloc[:,i],long,short,signal_span)\n",
    "    signals.index = returns.index\n",
    "    return signals.shift(1).fillna(0)\n",
    "\n",
    "\n",
    "# bollinger bands: mean reversion\n",
    "def rev(ret,lookback,band,plot):\n",
    "    \"\"\"\n",
    "    function with mean reversion bollinger bands\n",
    "    \"\"\"\n",
    "    # work with prices\n",
    "    p = (ret+1).cumprod()\n",
    "    # obtain mean & vol\n",
    "    m = p.rolling(window=lookback).mean()\n",
    "    s = p.rolling(window=lookback).std()\n",
    "    lower_band = m - band*s\n",
    "    higher_band = m + band*s\n",
    "    \n",
    "    signal2 = np.zeros(len(ret))\n",
    "    position_short = False\n",
    "    position_long = False\n",
    "    for i in range(len(ret)):\n",
    "        # sell if higher than band\n",
    "        if p[i] > higher_band[i]:\n",
    "            signal2[i] = -1\n",
    "            position_short = True\n",
    "        # sell back if from lower band to mean\n",
    "        elif p[i] < higher_band[i] and p.iloc[i] > m[i] and position_long == True:\n",
    "            signal2[i] = 0\n",
    "            position_long = False\n",
    "        # buy back if from higher band to mean\n",
    "        elif p[i] < m[i] and position_short == True:\n",
    "            signal2[i] = 0\n",
    "            position_short = False\n",
    "        # buy if lower barrier hit\n",
    "        elif p[i] < lower_band[i]:\n",
    "            signal2[i] = 1\n",
    "            position_long = True\n",
    "        else:\n",
    "            signal2[i] = signal2[i-1]\n",
    "        \n",
    "    return signal2\n",
    "\n",
    "\n",
    "def reversion_signals(returns,lookback=100,band=2,plot=False):\n",
    "    \"\"\"\n",
    "    calculates individual mean reversion based on bollinger bands\n",
    "    \"\"\"\n",
    "    signals = pd.DataFrame()\n",
    "    for i in range(returns.shape[1]):\n",
    "        signals['signal_{}'.format(i)] = rev(ret = returns.iloc[:,i],lookback=lookback,band=band,plot=plot)\n",
    "    signals.index = returns.index\n",
    "    return signals.shift(1).fillna(0)\n",
    "\n",
    "def pf_mean_reversion(returns, N=0.1):\n",
    "    \"\"\"\n",
    "    Khandani & Lo (2011) : buy yesterday's N% worst losers & sell N% best winners\n",
    "    \"\"\"\n",
    "    signals = pd.DataFrame(index=returns.index, columns=returns.columns)\n",
    "    for i in range(returns.shape[1]):\n",
    "        worst = returns.iloc[i,:].sort_values().index[:round(N*returns.shape[1])]\n",
    "        worst_ind = [returns.columns.get_loc(col) for col in worst]\n",
    "        best = returns.iloc[i,:].sort_values().index[-round(N*returns.shape[1]):]\n",
    "        best_ind = [returns.columns.get_loc(col) for col in best]\n",
    "        signals.iloc[i,worst_ind]  = 1\n",
    "        signals.iloc[i,best_ind] = -1\n",
    "    return signals.shift(1).fillna(0)\n",
    "\n",
    "def cointegrated_pairs(series):\n",
    "    \"\"\"\n",
    "    finds cointegrated pairs using engle-granger cointegration test for an input dataset\n",
    "    \"\"\"\n",
    "    n = series.shape[1]\n",
    "    keys = series.keys()\n",
    "    pairs = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            S1 = series[keys[i]]\n",
    "            S2 = series[keys[j]]\n",
    "            result = coint(S1, S2)\n",
    "            pvalue = result[1]\n",
    "            if pvalue < 0.025:\n",
    "                pairs.append((keys[i], keys[j]))\n",
    "    return pairs\n",
    "\n",
    "def pairs_signals(returns, window=252):\n",
    "    \"\"\"\n",
    "    pairs trading based on engle-granger cointegration test, signals are (inversely) proportional\n",
    "    to the zscore of the ratio of cointegrated pairs\n",
    "    \"\"\"\n",
    "    prices = (returns+1).cumprod()\n",
    "    signals = pd.DataFrame(np.zeros(prices.shape), columns=prices.columns)\n",
    "    for i in range(window,returns.shape[0],window):\n",
    "        pairs = cointegrated_pairs(prices.iloc[i-252:i,:])\n",
    "        for j in range(len(pairs)):\n",
    "            p1 = prices.columns.get_loc(pairs[j][0])\n",
    "            p2 = prices.columns.get_loc(pairs[j][1])\n",
    "            \n",
    "            # need 252 lookback for rolling mean\n",
    "            S1 = prices.iloc[(i-252):min(i+252,prices.shape[0]), p1]\n",
    "            S2 = prices.iloc[(i-252):min(i+252,prices.shape[0]), p2]\n",
    "            ratio = S1 / S2 # faster way to do pairs trading\n",
    "            \n",
    "            # calculate rolling mean\n",
    "            ratio_mean = ratio.rolling(252).mean()\n",
    "            ratio_std = ratio.rolling(252).std()\n",
    "            # technically mean should be backward looking -> data snooping here\n",
    "            # correct this later\n",
    "            # work with rolling\n",
    "            zscores = (ratio[252:] - ratio_mean[252:])/ratio_std[252:]\n",
    "            signals.iloc[i:min(i+252,prices.shape[0]),p1] = zscores\n",
    "            signals.iloc[i:min(i+252,prices.shape[0]),p2] = -zscores\n",
    "    return signals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "propose to define a divergence\n",
    "metric for each asset with the following formula β(ri,t −rf )−(rj,t −rf ) where β is the\n",
    "regression coefficient of ri on rj , ri is the ith asset return and rj is the weighted average\n",
    "of the 50 assets it correlates highest with. The top 10% are long and bottom 1% are\n",
    "shorted. There is a one-month look-back and one-month holding period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chen_divergence(returns):\n",
    "    \"\"\"\n",
    "    signal construction based on Chen et al. (2019) quasi-multivariate pairs trading strategy\n",
    "    \"\"\"\n",
    "    lookback = 3*returns.shape[1]\n",
    "    c = returns.rolling(lookback).corr()\n",
    "    signals = pd.DataFrame(np.zeros(returns.shape), columns=returns.columns)\n",
    "    \n",
    "    # loop : one-month look-back(or more like 3*number of columns) and one-month holding\n",
    "    for i in range(lookback, returns.shape[0], lookback):\n",
    "        c = returns.iloc[(i-lookback).]\n",
    "    # regress each on top X assets (5, 10 or 20)\n",
    "    # go long top 10% of predictions and short the bottom one\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Econometric\n",
    "   * Vector Autoregressive (VAR) -> DONE\n",
    "   * Vector error correction models (VECM)\n",
    "   * Kernel Ridge Regression (KRR) -> DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical tests\n",
    "   * **Granger causality** (column var causes row var) -> rejection means good candidate for MV\n",
    "       (tests the null hypothesis that the coefficients of past values in the regression equation is zero. In simpler terms, the past values of time series (X) do not cause the other series (Y))\n",
    "   * **ADF test** -> need stationarity in time series\n",
    "   * **cointegration** -> establish the presence of a statistically significant connection between two or more time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T16:23:33.972287Z",
     "start_time": "2022-02-04T16:23:33.946746Z"
    }
   },
   "outputs": [],
   "source": [
    "def grangers_causation_matrix(data, variables, test='ssr_chi2test', maxlag=4):    \n",
    "    \"\"\"\n",
    "    data      : pandas dataframe containing the time series variables\n",
    "    variables : list containing names of the time series variables.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    for c in df.columns:\n",
    "        for r in df.index:\n",
    "            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n",
    "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
    "            min_p_value = np.min(p_values)\n",
    "            df.loc[r, c] = min_p_value\n",
    "    df.columns = [var + '_x' for var in variables]\n",
    "    df.index = [var + '_y' for var in variables]\n",
    "    return df\n",
    "\n",
    "def adfuller_test(series, signif=0.05):\n",
    "    \"\"\"Perform ADFuller to test for Stationarity of given series of returns and print report\"\"\"\n",
    "    for name, column in series.iteritems():\n",
    "        r = adfuller(column, autolag='BIC')\n",
    "        output = {'test_statistic':round(r[0], 4), 'pvalue':round(r[1], 4), 'n_lags':round(r[2], 4), 'n_obs':r[3]}\n",
    "        p_value = output['pvalue']\n",
    "        if p_value > signif:\n",
    "            print(\"series {} is non-stationary\".format(column.name))\n",
    "            \n",
    "def cointegration_test(df, alpha=0.05): \n",
    "    \"\"\"Perform Johanson's Cointegration Test and Report Summary\"\"\"\n",
    "    out = coint_johansen(df,-1,5)\n",
    "    d = {'0.90':0, '0.95':1, '0.99':2}\n",
    "    traces = out.lr1\n",
    "    cvts = out.cvt[:, d[str(1-alpha)]]\n",
    "    # print output if failed\n",
    "    for name, trace, cvt in zip(df.columns, traces, cvts):\n",
    "        if trace < cvt: print(\"column {} failed the cointegration test\".format(name))\n",
    "\n",
    "def multivariate_tests(data,alpha=0.05,maxlag=4, test = 'ssr_chi2test'):\n",
    "    \"\"\"\n",
    "    data: pandas dataframe\n",
    "    alpha: significance level\n",
    "    maxlag: maximum number of lags for granger causality test\n",
    "    test: test for granger causality\n",
    "    \n",
    "    what it does: prints failed adf and cointegration tests\n",
    "    \n",
    "    returns: granger causality matrix\n",
    "    \"\"\"\n",
    "    adfuller_test(data, signif=alpha)\n",
    "    cointegration_test(data, alpha=alpha)\n",
    "    \n",
    "    return grangers_causation_matrix(data, data.columns, test=test, maxlag=maxlag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAR model\n",
    "still requires some code optimisation\n",
    "\n",
    "suggested improvements: build model using solely cointegrated time series -> can include more lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T16:23:34.097025Z",
     "start_time": "2022-02-04T16:23:34.060521Z"
    }
   },
   "outputs": [],
   "source": [
    "def var(df, window=30,maxlag=3,npredictions=5):\n",
    "    \n",
    "    n = len(df)\n",
    "    # initiate forecasts dataframe\n",
    "    forecasts = np.zeros((n+npredictions,df.shape[1])) \n",
    "    \n",
    "    # rolling window approach\n",
    "    for i in range(window,n,npredictions):\n",
    "        if i % 250 == 0: print('{}% done'.format(round(i/(n-30)*100)),end=\"\\r\")\n",
    "        bic_list = []\n",
    "        series = df.iloc[(i-window):i,:]\n",
    "        model = VAR(series)\n",
    "        \n",
    "        # optimal lags and fitting\n",
    "        for j in np.arange(0,maxlag+1,1):\n",
    "            try:\n",
    "                result = model.fit(j)\n",
    "                bic_list.append(result.bic)\n",
    "            except:\n",
    "                print(\"cannot deal with lag of {}, restarting with maxlag of {}\".format(j,maxlag-1))\n",
    "                if maxlag == 0:\n",
    "                    print(\"window size too small relative to number of assets, please restart\")\n",
    "                    return 0\n",
    "                else:\n",
    "                    return var(df, window, maxlag-1)\n",
    "        \n",
    "        # final check\n",
    "        if len(bic_list) == 1:\n",
    "                print(\"window size too small relative to number of assets, please restart\")\n",
    "                return 0   \n",
    "            \n",
    "        nbr_lags = bic_list.index(min(bic_list)) + 1\n",
    "        model_fitted = model.fit(nbr_lags)\n",
    "\n",
    "        forecast_input = series.values[-nbr_lags:]\n",
    "        \n",
    "        fc = model_fitted.forecast(y=forecast_input, steps=npredictions)\n",
    "        forecasts[i:(i+npredictions),:] = fc\n",
    "        \n",
    "    signals = np.where(forecasts > 0,1,-1)\n",
    "\n",
    "                \n",
    "    return signals\n",
    "\n",
    "def krr_signals_mv(data,window_size=30,npredictions=5):\n",
    "    n = len(data)\n",
    "\n",
    "    predicted_values = np.zeros((n+npredictions,data.shape[1]))\n",
    "    \n",
    "    # define x and y\n",
    "    x = data.shift(-1)\n",
    "    y = data\n",
    "\n",
    "    # rolling window, re-fit the model every \"npredictions\" days\n",
    "    for i in range(window_size,n-npredictions,npredictions):\n",
    "        if i % 100 == 0: print('{} % done'.format(round(i/(n-window_size)*100)),end=\"\\r\")\n",
    "        begin = i - window_size\n",
    "        \n",
    "        # obtain principal components, section 2.5\n",
    "        # question : is there no look-ahead bias by doing PCA on the same day returns, i.e. should it not be lagged?\n",
    "        pca = PCA(n_components=4)\n",
    "        principalComponents = pca.fit_transform(np.array(data.iloc[begin:i,:]))\n",
    "        \n",
    "        principalDf = pd.DataFrame(data = principalComponents, columns = ['pc1','pc2','pc3','pc4'])\n",
    "        \n",
    "        model = linear_model.LinearRegression()\n",
    "        model_score = model.fit(principalDf,data.iloc[begin:i,:]).score(principalDf,data.iloc[begin:i,:])\n",
    "        \n",
    "        #x = np.array(data.iloc[(i-window_size):i,:])#.reshape(-1,1)\n",
    "        #y = np.array(data.iloc[(i-window_size+1):(i+1),:]) \n",
    "        #x_pred = np.array(data.iloc[i:(i+npredictions),:])\n",
    "        \n",
    "        x_in = x.iloc[begin:i,:]\n",
    "        y_in = y.iloc[begin:i,:]\n",
    "        x_pred = x.iloc[i:(i+npredictions),:]\n",
    "        \n",
    "        # gaussian kernel parameters\n",
    "        nbr_assets = data.shape[1]\n",
    "        df = window_size - nbr_assets\n",
    "        if df<0:df=1 # for chi-square\n",
    "        r2 = np.array(model_score)\n",
    "        sigma0 = math.sqrt(chi2.ppf(0.95,df)) / math.pi\n",
    "        sigma = np.array([0.5*sigma0,sigma0,2*sigma0,4*sigma0,8*sigma0])\n",
    "        lambda0 = (1-r2)/(r2)\n",
    "        lambdaa = np.array([lambda0/8.0,lambda0/4.0,lambda0/2.0,lambda0,2*lambda0])\n",
    "        \n",
    "        parameters = {'kernel':['rbf'], 'alpha':lambdaa.tolist(),'degree':[2]}\n",
    "        clf = GridSearchCV(KernelRidge(), parameters)\n",
    "        clf = clf.fit(x_in,y_in)\n",
    "        pred = clf.predict(x_pred)\n",
    "        #print(i,i+npredictions)\n",
    "        predicted_values[(i):(i+npredictions),:] = pred\n",
    "    \n",
    "    # turn values into signals\n",
    "    signals = np.where(predicted_values > 0,1,-1)\n",
    "    \n",
    "    return signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN-based methodologies\n",
    "   * LSTM\n",
    "   * CNN\n",
    "   * other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (initial) Testing the strategies\n",
    "\n",
    "Conclusions: long only is much better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T16:23:35.268114Z",
     "start_time": "2022-02-04T16:23:35.260116Z"
    }
   },
   "outputs": [],
   "source": [
    "# performance evaluation\n",
    "def pnl(data,signals):\n",
    "    d = np.array(data)\n",
    "    s = np.array(signals)\n",
    "    return (pd.DataFrame(np.multiply(d,s), index=data.index)).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T17:13:52.080993Z",
     "start_time": "2022-02-04T17:13:52.009992Z"
    }
   },
   "outputs": [],
   "source": [
    "stocks = data\n",
    "# 1. test macd\n",
    "macd_multiple = macd_signals(stocks,26,9,9)\n",
    "print(\"macd done\")\n",
    "# 2. test mean reversion\n",
    "#m_r = reversion_signals(stocks)\n",
    "print(\"mean reversion done\")\n",
    "# 3. test krr univariate\n",
    "#krr_univ = krr_signals_univ(stocks)\n",
    "print(\"krr univariate done\")\n",
    "# 4. test krr multivariate\n",
    "#krr_mv = krr_signals_mv(stocks,window_size=15,npredictions=1)\n",
    "print(\"krr multivariate done\")\n",
    "# 5. test VAR -> cannot deal with very high dimensions\n",
    "#var_s = var(stocks.iloc[:,:5],window = 30,maxlag=3,npredictions=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T17:14:00.980611Z",
     "start_time": "2022-02-04T17:14:00.761450Z"
    }
   },
   "outputs": [],
   "source": [
    "macd_perf = pnl(stocks,macd_multiple)\n",
    "m_r_perf = pnl(stocks,m_r)\n",
    "krr_perf = pnl(stocks,krr_mv[:-1,:])\n",
    "#krr2_perf = pnl(stocks,krr_univ[:-5,:])\n",
    "var_perf = pnl(stocks.iloc[:,:5],var_s[:-5,:])\n",
    "momstr = pnl(data,momentum(data))\n",
    "\n",
    "# skip first 30 days -> window size\n",
    "macd_perf = macd_perf.iloc[30:,:]\n",
    "m_r_perf = m_r_perf.iloc[30:,:]\n",
    "krr_perf = krr_perf.iloc[30:,:]\n",
    "#krr2_perf = krr2_perf.iloc[30:,:]\n",
    "var_perf = var_perf.iloc[30:,:]\n",
    "momstr = momstr.iloc[30:,:]\n",
    "long_only = (stocks.iloc[30:,:]).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T17:14:03.484600Z",
     "start_time": "2022-02-04T17:14:03.333232Z"
    }
   },
   "outputs": [],
   "source": [
    "index = data.index[30:]\n",
    "%matplotlib widget\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(index,macd_perf.mean(axis=1), label = 'macd', color = 'black')\n",
    "ax.plot(index,momstr.mean(axis=1), label = 'momentum', color = 'r')\n",
    "\n",
    "#ax2 = ax.twinx()\n",
    "ax.plot(index,krr_perf.mean(axis=1), label = 'krr multivariate', color = 'b')\n",
    "#ax.plot(index,krr2_perf.mean(axis=1), label = 'krr univariate', color = 'b')\n",
    "ax.plot(index,m_r_perf.mean(axis=1), label = 'mean reversion', color = 'g')\n",
    "ax.plot(index,long_only.mean(axis=1), label = 'real', color = 'y')\n",
    "ax.plot(index,var_perf.mean(axis=1), label = 'var', color = 'gray')\n",
    "\n",
    "ax.legend(loc=0)\n",
    "#ax2.legend(loc=1)\n",
    "ax.grid()\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylabel(\"cumulative returns\")\n",
    "#ax2.set_ylabel(\"pred\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
